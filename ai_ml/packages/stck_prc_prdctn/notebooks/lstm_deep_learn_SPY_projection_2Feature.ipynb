{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da53d56-a54c-4862-a9e4-3fc26b0cca29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries loaded\n",
      "Number data points 5786 2001-01-02 to 2024-01-02\n",
      "****************************************************\n",
      "0      -0.771754\n",
      "1      -0.736606\n",
      "2      -0.744860\n",
      "3      -0.769623\n",
      "4      -0.763943\n",
      "          ...   \n",
      "5781    2.638665\n",
      "5782    2.646155\n",
      "5783    2.647722\n",
      "5784    2.635705\n",
      "5785    2.612540\n",
      "Name: Adjusted_Close_Price, Length: 5786, dtype: float64\n",
      "****************************************************\n",
      "5766 5766 20\n",
      "Batch 0: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 1: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 2: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "19\n",
      "Epoch[1/100] | loss train:0.072480, test:0.004076 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.028759, test:0.036715 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.022417, test:0.134881 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.023315, test:0.027298 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.021713, test:0.007368 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.021318, test:0.021037 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.021071, test:0.002329 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.019771, test:0.004467 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.020633, test:0.004433 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.017149, test:0.010453 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.017464, test:0.005258 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.017482, test:0.003122 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.017321, test:0.002310 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.016127, test:0.011910 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.014973, test:0.120813 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.019654, test:0.028774 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.017093, test:0.046290 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.014759, test:0.018114 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.013507, test:0.004421 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.014747, test:0.038681 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.014197, test:0.074489 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.013892, test:0.061662 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.014617, test:0.009696 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.013985, test:0.003108 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.012469, test:0.012695 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.013740, test:0.010240 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.013135, test:0.005951 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.013735, test:0.003278 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.014037, test:0.027410 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.017011, test:0.010697 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.014493, test:0.013023 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.012097, test:0.001772 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.012947, test:0.002657 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.011818, test:0.016377 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.013470, test:0.094688 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.012610, test:0.017403 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.012510, test:0.002321 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.011632, test:0.020687 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.012339, test:0.002356 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.013340, test:0.030493 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.009895, test:0.002873 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008262, test:0.014648 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008376, test:0.004010 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008220, test:0.006577 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.008414, test:0.002192 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007684, test:0.002554 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007640, test:0.004007 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.008312, test:0.002067 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008051, test:0.002582 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008224, test:0.004725 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007714, test:0.002320 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007367, test:0.003605 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007374, test:0.004265 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007217, test:0.002338 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.008030, test:0.002797 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007393, test:0.002719 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007611, test:0.003009 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.008316, test:0.005267 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.008001, test:0.002161 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007710, test:0.005064 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007991, test:0.006304 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007322, test:0.012401 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007817, test:0.046768 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007799, test:0.002212 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.007910, test:0.004917 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.008039, test:0.003222 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.007961, test:0.002187 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.007385, test:0.001956 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.007964, test:0.002850 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.007631, test:0.002122 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.007658, test:0.001841 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.007300, test:0.003386 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.008089, test:0.005043 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.007973, test:0.003595 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.007816, test:0.009681 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.007584, test:0.002226 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.007848, test:0.009693 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.007867, test:0.004543 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.007757, test:0.003162 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.007641, test:0.002641 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.007301, test:0.002067 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.007054, test:0.002942 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.007509, test:0.003178 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.007417, test:0.007158 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.007620, test:0.002034 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.007046, test:0.002880 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.007202, test:0.005597 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.007437, test:0.001898 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.007231, test:0.002049 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.007854, test:0.002127 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.010136, test:0.002815 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.007373, test:0.002055 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.007388, test:0.008605 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.007174, test:0.006544 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.007379, test:0.001917 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.007371, test:0.004544 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.006854, test:0.008302 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.007147, test:0.002964 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.006911, test:0.002710 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.007184, test:0.001999 | lr:0.000100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from matplotlib.pyplot import figure\n",
    "import random\n",
    "# Set random seed for reproducibility\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "print(\"All libraries loaded\")\n",
    "########################################################################################################################################################\n",
    "config = {\n",
    "    \"alpha_vantage\": {\n",
    "        \"key\": \"demo\", # you can use the demo API key for this project, but please make sure to get your own API key at https://www.alphavantage.co/support/#api-key\n",
    "        \"symbol\": \"SPY\",\n",
    "        \"outputsize\": \"full\",\n",
    "        \"key_adjusted_close\": \"5. adjusted close\",\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"window_size\": 20,\n",
    "        \"train_split_size\": 0.90,\n",
    "    }, \n",
    "    \"plots\": {\n",
    "        \"xticks_interval\": 90, # show a date every 90 days\n",
    "        \"color_actual\": \"#001f3f\",\n",
    "        \"color_train\": \"#3D9970\",\n",
    "        \"color_val\": \"#0074D9\",\n",
    "        \"color_pred_train\": \"#3D9970\",\n",
    "        \"color_pred_val\": \"#0074D9\",\n",
    "        \"color_pred_test\": \"#FF4136\",\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"input_size\": 1, # since we are only using 1 feature, close price\n",
    "        \"num_lstm_layers\": 2,\n",
    "        \"lstm_size\": 64,\n",
    "        \"dropout\": 0.2,\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"device\": \"cpu\", # \"cuda\" or \"cpu\"\n",
    "        \"batch_size\": 32,\n",
    "        \"num_epoch\": 100,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"scheduler_step_size\": 40,\n",
    "    }\n",
    "}\n",
    "def download_data(config):\n",
    "    # Replace 'SPY' with the ticker symbol of the stock you want to fetch\n",
    "        ticker_symbol = 'SPY'\n",
    "\n",
    "    # Fetch historical data from Yahoo Finance\n",
    "        #df = yf.download(ticker_symbol, start='2001-01-01', end='2023-10-23')\n",
    "    \n",
    "    # Extract date in yyyy-mm-dd format\n",
    "        #df['Date'] = df.index.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Extract adjusted close price\n",
    "        #df['Adjusted_Close_Price'] = df['Adj Close']\n",
    "    # Store the data in an Excel file\n",
    "        #df.to_excel('spy_data_lstm_yahoo.xlsx', index=False)\n",
    "\n",
    "    # Read the data from the Excel file\n",
    "        df = pd.read_excel('spy_data_lstm_yahoo.xlsx')\n",
    "\n",
    "    # Find the total number of data points\n",
    "        num_data_points = len(df)\n",
    "  \n",
    "    # Find the date range (start and end dates)\n",
    "        data_date = df['Date']\n",
    "        data_close_price = df['Adjusted_Close_Price']\n",
    "        start_date = df['Date'].iloc[0]\n",
    "        end_date = df['Date'].iloc[-1]\n",
    "        display_date_range = f\"{start_date} to {end_date}\"\n",
    "        print(\"Number data points\", num_data_points, display_date_range)\n",
    "\n",
    "        return data_date, data_close_price, num_data_points, display_date_range\n",
    "\n",
    "data_date, data_close_price, num_data_points, display_date_range = download_data(config)\n",
    "\n",
    "\n",
    "# plot\n",
    "\n",
    "fig = figure(figsize=(25, 5), dpi=80)\n",
    "fig.patch.set_facecolor((1.0, 1.0, 1.0))\n",
    "plt.plot(data_date, data_close_price, color=config[\"plots\"][\"color_actual\"])\n",
    "xticks = [data_date[i] if ((i%config[\"plots\"][\"xticks_interval\"]==0 and (num_data_points-i) > config[\"plots\"][\"xticks_interval\"]) or i==num_data_points-1) else None for i in range(num_data_points)] # make x ticks nice\n",
    "x = np.arange(0,len(xticks))\n",
    "plt.xticks(x, xticks, rotation='vertical')\n",
    "plt.title(\"Daily close price for \" + config[\"alpha_vantage\"][\"symbol\"] + \", \" + display_date_range)\n",
    "plt.grid(which='major', axis='y', linestyle='--')\n",
    "#plt.show()\n",
    "\n",
    "# Block-2::Data preparation: normalizing raw financial data\n",
    "# normalize\n",
    "class Normalizer():\n",
    "    def __init__(self):\n",
    "        self.mu = None\n",
    "        self.sd = None\n",
    "\n",
    "    def fit_transform(self, x):\n",
    "        self.mu = np.mean(x, axis=(0))\n",
    "        self.sd = np.std(x, axis=(0))\n",
    "        normalized_x = (x - self.mu)/self.sd\n",
    "        return normalized_x\n",
    "\n",
    "    def inverse_transform(self, x):\n",
    "        return (x*self.sd) + self.mu\n",
    "        \n",
    "scaler = Normalizer()\n",
    "normalized_data_close_price = scaler.fit_transform(data_close_price)\n",
    "\n",
    "print(\"****************************************************\")\n",
    "print(normalized_data_close_price)\n",
    "print(\"****************************************************\")\n",
    "\n",
    "def prepare_data_x(x, window_size):\n",
    "    # perform windowing\n",
    "    # Ensure that x is a NumPy array\n",
    "    x = normalized_data_close_price.values\n",
    "    n_row = x.shape[0] - window_size + 1\n",
    "    output = np.lib.stride_tricks.as_strided(x, shape=(n_row, window_size), strides=(x.strides[0], x.strides[0]))\n",
    "            \n",
    "    return output[:-1], output[-1]\n",
    "\n",
    "\n",
    "def prepare_data_y(x, window_size):\n",
    "    # # perform simple moving average\n",
    "    # output = np.convolve(x, np.ones(window_size), 'valid') / window_size\n",
    "    # Ensure that x is a NumPy array\n",
    "    x = normalized_data_close_price.values\n",
    "    # use the next day as label\n",
    "    output = x[window_size:]\n",
    "    return output\n",
    "\n",
    "data_x, data_x_unseen = prepare_data_x(normalized_data_close_price, window_size=config[\"data\"][\"window_size\"])\n",
    "data_y = prepare_data_y(normalized_data_close_price, window_size=config[\"data\"][\"window_size\"])\n",
    "print(len(data_x),len(data_y),len(data_x_unseen))\n",
    "#print(normalized_data_close_price.head(21))\n",
    "# split dataset\n",
    "\n",
    "split_index = int(data_y.shape[0]*config[\"data\"][\"train_split_size\"])\n",
    "data_x_train = data_x[:split_index]\n",
    "data_x_val = data_x[split_index:]\n",
    "data_y_train = data_y[:split_index]\n",
    "data_y_val = data_y[split_index:]\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        x = np.expand_dims(x, 2) # in our case, we have only 1 feature, so we need to convert `x` into [batch, sequence, features] for LSTM\n",
    "        self.x = x.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x[idx], self.y[idx])\n",
    "\n",
    "dataset_train = TimeSeriesDataset(data_x_train, data_y_train)\n",
    "dataset_val = TimeSeriesDataset(data_x_val, data_y_val)\n",
    "\n",
    "#print(\"Train data shape\", dataset_train.x.shape, dataset_train.y.shape)\n",
    "#print(\"Validation data shape\", dataset_val.x.shape, dataset_val.y.shape)\n",
    "\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=config[\"training\"][\"batch_size\"], shuffle=True)\n",
    "val_dataloader = DataLoader(dataset_val, batch_size=config[\"training\"][\"batch_size\"], shuffle=True)\n",
    "\n",
    "#Iterate Through Data Loaders:\n",
    "#print(\"Iterate Through Data Loaders:\")\n",
    "for batch_idx, (input_data, target_data) in enumerate(train_dataloader):\n",
    "        #print(f\"Batch {batch_idx}: Input Shape: {input_data.shape}, Target Shape: {target_data.shape}\")\n",
    "        if batch_idx < 3:\n",
    "            print(f\"Batch {batch_idx}: Input Shape: {input_data.shape}, Target Shape: {target_data.shape}\")\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=64, num_layers=2, output_size=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        self.linear_1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm = nn.LSTM(hidden_layer_size, hidden_size=self.hidden_layer_size, num_layers=num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(num_layers*hidden_layer_size, output_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                 nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                 nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                 nn.init.orthogonal_(param)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "\n",
    "        # layer 1\n",
    "        x = self.linear_1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # LSTM layer\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "\n",
    "        # reshape output from hidden cell into [batch, features] for `linear_2`\n",
    "        x = h_n.permute(1, 0, 2).reshape(batchsize, -1) \n",
    "        \n",
    "        # layer 2\n",
    "        x = self.dropout(x)\n",
    "        predictions = self.linear_2(x)\n",
    "        return predictions[:,-1]\n",
    "\n",
    "def run_epoch(dataloader, is_training=False):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    if is_training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    for idx, (x, y) in enumerate(dataloader):\n",
    "        if is_training:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        batchsize = x.shape[0]\n",
    "\n",
    "        x = x.to(config[\"training\"][\"device\"])\n",
    "        y = y.to(config[\"training\"][\"device\"])\n",
    "\n",
    "        out = model(x)\n",
    "        loss = criterion(out.contiguous(), y.contiguous())\n",
    "\n",
    "        if is_training:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss += (loss.detach().item() / batchsize)\n",
    "\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "    return epoch_loss, lr\n",
    "\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=config[\"training\"][\"batch_size\"], shuffle=True)\n",
    "\n",
    "#print(len(train_dataloader))\n",
    "for batch in train_dataloader:\n",
    "    # Process the batch\n",
    "    x, y = batch\n",
    "    #print(x.shape, y.shape)\n",
    "\n",
    "val_dataloader = DataLoader(dataset_val, batch_size=config[\"training\"][\"batch_size\"], shuffle=True)\n",
    "print(len(val_dataloader))\n",
    "for batch in val_dataloader:\n",
    "    # Process the batch\n",
    "    x, y = batch\n",
    "    #print(x.shape, y.shape)\n",
    "\n",
    "model = LSTMModel(input_size=config[\"model\"][\"input_size\"], hidden_layer_size=config[\"model\"][\"lstm_size\"], num_layers=config[\"model\"][\"num_lstm_layers\"], output_size=1, dropout=config[\"model\"][\"dropout\"])\n",
    "model = model.to(config[\"training\"][\"device\"])\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"training\"][\"learning_rate\"], betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=config[\"training\"][\"scheduler_step_size\"], gamma=0.1)\n",
    "\n",
    "for epoch in range(config[\"training\"][\"num_epoch\"]):\n",
    "    loss_train, lr_train = run_epoch(train_dataloader, is_training=True)\n",
    "    loss_val, lr_val = run_epoch(val_dataloader)\n",
    "    scheduler.step()\n",
    "    \n",
    "    print('Epoch[{}/{}] | loss train:{:.6f}, test:{:.6f} | lr:{:.6f}'\n",
    "              .format(epoch+1, config[\"training\"][\"num_epoch\"], loss_train, loss_val, lr_train))\n",
    "\n",
    "# here we re-initialize dataloader so the data doesn't shuffled, so we can plot the values by date\n",
    "\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=config[\"training\"][\"batch_size\"], shuffle=False)\n",
    "val_dataloader = DataLoader(dataset_val, batch_size=config[\"training\"][\"batch_size\"], shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# predict on the training data, to see how well the model managed to learn and memorize\n",
    "\n",
    "predicted_train = np.array([])\n",
    "\n",
    "for idx, (x, y) in enumerate(train_dataloader):\n",
    "    x = x.to(config[\"training\"][\"device\"])\n",
    "    out = model(x)\n",
    "    out = out.cpu().detach().numpy()\n",
    "    predicted_train = np.concatenate((predicted_train, out))\n",
    "\n",
    "# predict on the validation data, to see how the model does\n",
    "\n",
    "predicted_val = np.array([])\n",
    "\n",
    "for idx, (x, y) in enumerate(val_dataloader):\n",
    "    x = x.to(config[\"training\"][\"device\"])\n",
    "    out = model(x)\n",
    "    out = out.cpu().detach().numpy()\n",
    "    predicted_val = np.concatenate((predicted_val, out))\n",
    "\n",
    "# predict the closing price of the next trading day\n",
    "\n",
    "model.eval()\n",
    "\n",
    "x = torch.tensor(data_x_unseen).float().to(config[\"training\"][\"device\"]).unsqueeze(0).unsqueeze(2) # this is the data type and shape required, [batch, sequence, feature]\n",
    "prediction = model(x)\n",
    "prediction = prediction.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "print(\"Predicted close price of the next trading day:\", round(to_plot_data_y_test_pred[plot_range-1], 2))\n",
    "\n",
    "# Prepare data for the DataFrame\n",
    "data = {\n",
    "    'date': data_date[config[\"data\"][\"window_size\"] + split_index:],  # Adjust the starting index to align with the predictions\n",
    "    'actual_close_price': scaler.inverse_transform(data_y_val),\n",
    "    'predicted_price': scaler.inverse_transform(predicted_val),\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate the difference between actual and predicted prices\n",
    "df['difference'] = df['actual_close_price'] - df['predicted_price']\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.tail(5))\n",
    "df.to_excel('spy_lstm_predictions.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f68bad7-f037-43de-b9ae-9f1a05207557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
