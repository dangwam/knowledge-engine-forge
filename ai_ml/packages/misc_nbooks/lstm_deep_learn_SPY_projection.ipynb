{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45c02b2-e820-4941-9b86-8d2988c6913a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries loaded\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "Number data points 5786 2001-01-02 to 2024-01-02\n",
      "****************************************************\n",
      "Date\n",
      "2001-01-02   -0.771754\n",
      "2001-01-03   -0.736606\n",
      "2001-01-04   -0.744860\n",
      "2001-01-05   -0.769623\n",
      "2001-01-08   -0.763943\n",
      "                ...   \n",
      "2023-12-26    2.638665\n",
      "2023-12-27    2.646155\n",
      "2023-12-28    2.647722\n",
      "2023-12-29    2.635705\n",
      "2024-01-02    2.612540\n",
      "Name: Adjusted_Close_Price, Length: 5786, dtype: float64\n",
      "****************************************************\n",
      "5766 5766 20\n",
      "Train data shape (5189, 20, 1) (5189,)\n",
      "Validation data shape (577, 20, 1) (577,)\n",
      "Iterate Through Data Loaders:\n",
      "Batch 0: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 0: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 1: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 1: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 2: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 2: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 3: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 4: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 5: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 6: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 7: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 8: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 9: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 10: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 11: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 12: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 13: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 14: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 15: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 16: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 17: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 18: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 19: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 20: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 21: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 22: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 23: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 24: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 25: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 26: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 27: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 28: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 29: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 30: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 31: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 32: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 33: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 34: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 35: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 36: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 37: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 38: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 39: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 40: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 41: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 42: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 43: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 44: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 45: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 46: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 47: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 48: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 49: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 50: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 51: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 52: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 53: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 54: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 55: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 56: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 57: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 58: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 59: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 60: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 61: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 62: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 63: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 64: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 65: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 66: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 67: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 68: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 69: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 70: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 71: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 72: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 73: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 74: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 75: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 76: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 77: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 78: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 79: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 80: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 81: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 82: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 83: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 84: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 85: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 86: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 87: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 88: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 89: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 90: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 91: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 92: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 93: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 94: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 95: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 96: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 97: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 98: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 99: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 100: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 101: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 102: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 103: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 104: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 105: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 106: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 107: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 108: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 109: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 110: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 111: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 112: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 113: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 114: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 115: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 116: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 117: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 118: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 119: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 120: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 121: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 122: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 123: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 124: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 125: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 126: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 127: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 128: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 129: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 130: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 131: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 132: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 133: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 134: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 135: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 136: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 137: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 138: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 139: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 140: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 141: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 142: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 143: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 144: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 145: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 146: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 147: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 148: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 149: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 150: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 151: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 152: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 153: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 154: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 155: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 156: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 157: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 158: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 159: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 160: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 161: Input Shape: torch.Size([32, 20, 1]), Target Shape: torch.Size([32])\n",
      "Batch 162: Input Shape: torch.Size([5, 20, 1]), Target Shape: torch.Size([5])\n",
      "19\n",
      "Epoch[1/100] | loss train:0.072480, test:0.004076 | lr:0.010000\n",
      "Epoch[2/100] | loss train:0.028759, test:0.036714 | lr:0.010000\n",
      "Epoch[3/100] | loss train:0.022417, test:0.134884 | lr:0.010000\n",
      "Epoch[4/100] | loss train:0.023315, test:0.027353 | lr:0.010000\n",
      "Epoch[5/100] | loss train:0.021717, test:0.007286 | lr:0.010000\n",
      "Epoch[6/100] | loss train:0.021334, test:0.021325 | lr:0.010000\n",
      "Epoch[7/100] | loss train:0.021195, test:0.002626 | lr:0.010000\n",
      "Epoch[8/100] | loss train:0.019802, test:0.007035 | lr:0.010000\n",
      "Epoch[9/100] | loss train:0.018867, test:0.006652 | lr:0.010000\n",
      "Epoch[10/100] | loss train:0.017013, test:0.030704 | lr:0.010000\n",
      "Epoch[11/100] | loss train:0.017502, test:0.004892 | lr:0.010000\n",
      "Epoch[12/100] | loss train:0.017176, test:0.002742 | lr:0.010000\n",
      "Epoch[13/100] | loss train:0.017810, test:0.004815 | lr:0.010000\n",
      "Epoch[14/100] | loss train:0.016010, test:0.007193 | lr:0.010000\n",
      "Epoch[15/100] | loss train:0.014732, test:0.118679 | lr:0.010000\n",
      "Epoch[16/100] | loss train:0.019661, test:0.024377 | lr:0.010000\n",
      "Epoch[17/100] | loss train:0.017216, test:0.045472 | lr:0.010000\n",
      "Epoch[18/100] | loss train:0.014536, test:0.014211 | lr:0.010000\n",
      "Epoch[19/100] | loss train:0.013318, test:0.007075 | lr:0.010000\n",
      "Epoch[20/100] | loss train:0.014741, test:0.038162 | lr:0.010000\n",
      "Epoch[21/100] | loss train:0.014602, test:0.071354 | lr:0.010000\n",
      "Epoch[22/100] | loss train:0.013884, test:0.060389 | lr:0.010000\n",
      "Epoch[23/100] | loss train:0.015228, test:0.001493 | lr:0.010000\n",
      "Epoch[24/100] | loss train:0.013697, test:0.006702 | lr:0.010000\n",
      "Epoch[25/100] | loss train:0.013043, test:0.005429 | lr:0.010000\n",
      "Epoch[26/100] | loss train:0.013633, test:0.008984 | lr:0.010000\n",
      "Epoch[27/100] | loss train:0.013299, test:0.009791 | lr:0.010000\n",
      "Epoch[28/100] | loss train:0.013474, test:0.001482 | lr:0.010000\n",
      "Epoch[29/100] | loss train:0.013980, test:0.027435 | lr:0.010000\n",
      "Epoch[30/100] | loss train:0.017987, test:0.008177 | lr:0.010000\n",
      "Epoch[31/100] | loss train:0.014950, test:0.009232 | lr:0.010000\n",
      "Epoch[32/100] | loss train:0.012127, test:0.002962 | lr:0.010000\n",
      "Epoch[33/100] | loss train:0.012961, test:0.006519 | lr:0.010000\n",
      "Epoch[34/100] | loss train:0.012056, test:0.022799 | lr:0.010000\n",
      "Epoch[35/100] | loss train:0.013124, test:0.096128 | lr:0.010000\n",
      "Epoch[36/100] | loss train:0.012650, test:0.004363 | lr:0.010000\n",
      "Epoch[37/100] | loss train:0.012371, test:0.002449 | lr:0.010000\n",
      "Epoch[38/100] | loss train:0.011837, test:0.011145 | lr:0.010000\n",
      "Epoch[39/100] | loss train:0.012381, test:0.001787 | lr:0.010000\n",
      "Epoch[40/100] | loss train:0.012844, test:0.037223 | lr:0.010000\n",
      "Epoch[41/100] | loss train:0.009903, test:0.002395 | lr:0.001000\n",
      "Epoch[42/100] | loss train:0.008306, test:0.017145 | lr:0.001000\n",
      "Epoch[43/100] | loss train:0.008349, test:0.005296 | lr:0.001000\n",
      "Epoch[44/100] | loss train:0.008102, test:0.003966 | lr:0.001000\n",
      "Epoch[45/100] | loss train:0.008314, test:0.001852 | lr:0.001000\n",
      "Epoch[46/100] | loss train:0.007623, test:0.001622 | lr:0.001000\n",
      "Epoch[47/100] | loss train:0.007641, test:0.003453 | lr:0.001000\n",
      "Epoch[48/100] | loss train:0.008283, test:0.002632 | lr:0.001000\n",
      "Epoch[49/100] | loss train:0.008040, test:0.001551 | lr:0.001000\n",
      "Epoch[50/100] | loss train:0.008371, test:0.003429 | lr:0.001000\n",
      "Epoch[51/100] | loss train:0.007692, test:0.001503 | lr:0.001000\n",
      "Epoch[52/100] | loss train:0.007351, test:0.003878 | lr:0.001000\n",
      "Epoch[53/100] | loss train:0.007456, test:0.002777 | lr:0.001000\n",
      "Epoch[54/100] | loss train:0.007198, test:0.003185 | lr:0.001000\n",
      "Epoch[55/100] | loss train:0.008022, test:0.002413 | lr:0.001000\n",
      "Epoch[56/100] | loss train:0.007338, test:0.002372 | lr:0.001000\n",
      "Epoch[57/100] | loss train:0.007540, test:0.002338 | lr:0.001000\n",
      "Epoch[58/100] | loss train:0.008311, test:0.004310 | lr:0.001000\n",
      "Epoch[59/100] | loss train:0.007911, test:0.002129 | lr:0.001000\n",
      "Epoch[60/100] | loss train:0.007740, test:0.004246 | lr:0.001000\n",
      "Epoch[61/100] | loss train:0.007914, test:0.008937 | lr:0.001000\n",
      "Epoch[62/100] | loss train:0.007270, test:0.012127 | lr:0.001000\n",
      "Epoch[63/100] | loss train:0.007700, test:0.015343 | lr:0.001000\n",
      "Epoch[64/100] | loss train:0.007768, test:0.002227 | lr:0.001000\n",
      "Epoch[65/100] | loss train:0.007856, test:0.003162 | lr:0.001000\n",
      "Epoch[66/100] | loss train:0.008013, test:0.002821 | lr:0.001000\n",
      "Epoch[67/100] | loss train:0.007915, test:0.001684 | lr:0.001000\n",
      "Epoch[68/100] | loss train:0.007389, test:0.001513 | lr:0.001000\n",
      "Epoch[69/100] | loss train:0.007956, test:0.002773 | lr:0.001000\n",
      "Epoch[70/100] | loss train:0.007550, test:0.001714 | lr:0.001000\n",
      "Epoch[71/100] | loss train:0.007639, test:0.001390 | lr:0.001000\n",
      "Epoch[72/100] | loss train:0.007254, test:0.002616 | lr:0.001000\n",
      "Epoch[73/100] | loss train:0.008011, test:0.004061 | lr:0.001000\n",
      "Epoch[74/100] | loss train:0.007791, test:0.003265 | lr:0.001000\n",
      "Epoch[75/100] | loss train:0.007901, test:0.005853 | lr:0.001000\n",
      "Epoch[76/100] | loss train:0.007525, test:0.003140 | lr:0.001000\n",
      "Epoch[77/100] | loss train:0.007836, test:0.003608 | lr:0.001000\n",
      "Epoch[78/100] | loss train:0.007744, test:0.003727 | lr:0.001000\n",
      "Epoch[79/100] | loss train:0.007817, test:0.006097 | lr:0.001000\n",
      "Epoch[80/100] | loss train:0.007559, test:0.001865 | lr:0.001000\n",
      "Epoch[81/100] | loss train:0.007262, test:0.001672 | lr:0.000100\n",
      "Epoch[82/100] | loss train:0.006985, test:0.002380 | lr:0.000100\n",
      "Epoch[83/100] | loss train:0.007510, test:0.003390 | lr:0.000100\n",
      "Epoch[84/100] | loss train:0.007329, test:0.006656 | lr:0.000100\n",
      "Epoch[85/100] | loss train:0.007639, test:0.001572 | lr:0.000100\n",
      "Epoch[86/100] | loss train:0.007068, test:0.001956 | lr:0.000100\n",
      "Epoch[87/100] | loss train:0.007193, test:0.004356 | lr:0.000100\n",
      "Epoch[88/100] | loss train:0.007415, test:0.001432 | lr:0.000100\n",
      "Epoch[89/100] | loss train:0.007228, test:0.001980 | lr:0.000100\n",
      "Epoch[90/100] | loss train:0.007790, test:0.002178 | lr:0.000100\n",
      "Epoch[91/100] | loss train:0.009927, test:0.001646 | lr:0.000100\n",
      "Epoch[92/100] | loss train:0.007363, test:0.001541 | lr:0.000100\n",
      "Epoch[93/100] | loss train:0.007378, test:0.005015 | lr:0.000100\n",
      "Epoch[94/100] | loss train:0.007180, test:0.002890 | lr:0.000100\n",
      "Epoch[95/100] | loss train:0.007317, test:0.001441 | lr:0.000100\n",
      "Epoch[96/100] | loss train:0.007349, test:0.001676 | lr:0.000100\n",
      "Epoch[97/100] | loss train:0.006836, test:0.009165 | lr:0.000100\n",
      "Epoch[98/100] | loss train:0.007167, test:0.002405 | lr:0.000100\n",
      "Epoch[99/100] | loss train:0.006877, test:0.002946 | lr:0.000100\n",
      "Epoch[100/100] | loss train:0.007196, test:0.001447 | lr:0.000100\n",
      "evaluate model-start\n",
      "evaluate model-end\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "train_dataloader\n",
      "val_dataloader\n",
      "val_dataloader\n",
      "val_dataloader\n",
      "val_dataloader\n",
      "val_dataloader\n",
      "val_dataloader\n",
      "val_dataloader\n",
      "val_dataloader\n",
      "val_dataloader\n",
      "val_dataloader\n",
      "val_dataloader\n",
      "val_dataloader\n",
      "val_dataloader\n",
      "val_dataloader\n",
      "val_dataloader\n",
      "val_dataloader\n",
      "val_dataloader\n",
      "val_dataloader\n",
      "val_dataloader\n",
      "----plottings--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MAYANK DANGWAL\\AppData\\Local\\Temp\\ipykernel_18816\\3528531142.py:336: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  xticks = [data_date[i] if ((i%config[\"plots\"][\"xticks_interval\"]==0 and (num_data_points-i) > config[\"plots\"][\"xticks_interval\"]) or i==num_data_points-1) else None for i in range(num_data_points)] # make x ticks nice\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from matplotlib.pyplot import figure\n",
    "import random\n",
    "# Set random seed for reproducibility\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "print(\"All libraries loaded\")\n",
    "########################################################################################################################################################\n",
    "config = {\n",
    "    \"alpha_vantage\": {\n",
    "        \"key\": \"demo\", # you can use the demo API key for this project, but please make sure to get your own API key at https://www.alphavantage.co/support/#api-key\n",
    "        \"symbol\": \"SPY\",\n",
    "        \"outputsize\": \"full\",\n",
    "        \"key_adjusted_close\": \"5. adjusted close\",\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"window_size\": 20,\n",
    "        \"train_split_size\": 0.90,\n",
    "    }, \n",
    "    \"plots\": {\n",
    "        \"xticks_interval\": 90, # show a date every 90 days\n",
    "        \"color_actual\": \"#001f3f\",\n",
    "        \"color_train\": \"#3D9970\",\n",
    "        \"color_val\": \"#0074D9\",\n",
    "        \"color_pred_train\": \"#3D9970\",\n",
    "        \"color_pred_val\": \"#0074D9\",\n",
    "        \"color_pred_test\": \"#FF4136\",\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"input_size\": 1, # since we are only using 1 feature, close price\n",
    "        \"num_lstm_layers\": 2,\n",
    "        \"lstm_size\": 64,\n",
    "        \"dropout\": 0.2,\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"device\": \"cpu\", # \"cuda\" or \"cpu\"\n",
    "        \"batch_size\": 32,\n",
    "        \"num_epoch\": 100,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"scheduler_step_size\": 40,\n",
    "    }\n",
    "}\n",
    "\n",
    "def download_data(config):\n",
    "    # Replace 'SPY' with the ticker symbol of the stock you want to fetch\n",
    "        ticker_symbol = 'SPY'\n",
    "\n",
    "    # Fetch historical data from Yahoo Finance\n",
    "        df = yf.download(ticker_symbol, start='2001-01-01', end='2024-01-03')\n",
    "    \n",
    "    # Extract date in yyyy-mm-dd format\n",
    "        df['Date'] = df.index.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Extract adjusted close price\n",
    "        df['Adjusted_Close_Price'] = df['Adj Close']\n",
    "    # Store the data in an Excel file\n",
    "        df.to_excel('spy_data_lstm_yahoo.xlsx', index=False)\n",
    "\n",
    "    # Read the data from the Excel file\n",
    "        #df = pd.read_excel('spy_data_lstm_yahoo.xlsx')\n",
    "\n",
    "    # Find the total number of data points\n",
    "        num_data_points = len(df)\n",
    "  \n",
    "    # Find the date range (start and end dates)\n",
    "        data_date = df['Date']\n",
    "        data_close_price = df['Adjusted_Close_Price']\n",
    "        start_date = df['Date'].iloc[0]\n",
    "        end_date = df['Date'].iloc[-1]\n",
    "        display_date_range = f\"{start_date} to {end_date}\"\n",
    "        print(\"Number data points\", num_data_points, display_date_range)\n",
    "\n",
    "        return data_date, data_close_price, num_data_points, display_date_range\n",
    "\n",
    "data_date, data_close_price, num_data_points, display_date_range = download_data(config)\n",
    "\n",
    "'''\n",
    "# plot\n",
    "\n",
    "fig = figure(figsize=(25, 5), dpi=80)\n",
    "fig.patch.set_facecolor((1.0, 1.0, 1.0))\n",
    "plt.plot(data_date, data_close_price, color=config[\"plots\"][\"color_actual\"])\n",
    "xticks = [data_date[i] if ((i%config[\"plots\"][\"xticks_interval\"]==0 and (num_data_points-i) > config[\"plots\"][\"xticks_interval\"]) or i==num_data_points-1) else None for i in range(num_data_points)] # make x ticks nice\n",
    "x = np.arange(0,len(xticks))\n",
    "plt.xticks(x, xticks, rotation='vertical')\n",
    "plt.title(\"Daily close price for \" + config[\"alpha_vantage\"][\"symbol\"] + \", \" + display_date_range)\n",
    "plt.grid(which='major', axis='y', linestyle='--')\n",
    "#plt.show()\n",
    "'''\n",
    "# Block-2::Data preparation: normalizing raw financial data\n",
    "# normalize\n",
    "class Normalizer():\n",
    "    def __init__(self):\n",
    "        self.mu = None\n",
    "        self.sd = None\n",
    "\n",
    "    def fit_transform(self, x):\n",
    "        self.mu = np.mean(x, axis=(0))\n",
    "        self.sd = np.std(x, axis=(0))\n",
    "        normalized_x = (x - self.mu)/self.sd\n",
    "        return normalized_x\n",
    "\n",
    "    def inverse_transform(self, x):\n",
    "        return (x*self.sd) + self.mu\n",
    "        \n",
    "scaler = Normalizer()\n",
    "normalized_data_close_price = scaler.fit_transform(data_close_price)\n",
    "\n",
    "print(\"****************************************************\")\n",
    "print(normalized_data_close_price)\n",
    "print(\"****************************************************\")\n",
    "\n",
    "def prepare_data_x(x, window_size):\n",
    "    # perform windowing\n",
    "    # Ensure that x is a NumPy array\n",
    "    x = normalized_data_close_price.values\n",
    "    n_row = x.shape[0] - window_size + 1\n",
    "    output = np.lib.stride_tricks.as_strided(x, shape=(n_row, window_size), strides=(x.strides[0], x.strides[0]))\n",
    "            \n",
    "    return output[:-1], output[-1]\n",
    "\n",
    "\n",
    "def prepare_data_y(x, window_size):\n",
    "    # # perform simple moving average\n",
    "    # output = np.convolve(x, np.ones(window_size), 'valid') / window_size\n",
    "    # Ensure that x is a NumPy array\n",
    "    x = normalized_data_close_price.values\n",
    "    # use the next day as label\n",
    "    output = x[window_size:]\n",
    "    return output\n",
    "\n",
    "data_x, data_x_unseen = prepare_data_x(normalized_data_close_price, window_size=config[\"data\"][\"window_size\"])\n",
    "data_y = prepare_data_y(normalized_data_close_price, window_size=config[\"data\"][\"window_size\"])\n",
    "print(len(data_x),len(data_y),len(data_x_unseen))\n",
    "#print(normalized_data_close_price.head(21))\n",
    "# split dataset\n",
    "\n",
    "split_index = int(data_y.shape[0]*config[\"data\"][\"train_split_size\"])\n",
    "data_x_train = data_x[:split_index]\n",
    "data_x_val = data_x[split_index:]\n",
    "data_y_train = data_y[:split_index]\n",
    "data_y_val = data_y[split_index:]\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        x = np.expand_dims(x, 2) # in our case, we have only 1 feature, so we need to convert `x` into [batch, sequence, features] for LSTM\n",
    "        self.x = x.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x[idx], self.y[idx])\n",
    "\n",
    "dataset_train = TimeSeriesDataset(data_x_train, data_y_train)\n",
    "dataset_val = TimeSeriesDataset(data_x_val, data_y_val)\n",
    "\n",
    "print(\"Train data shape\", dataset_train.x.shape, dataset_train.y.shape)\n",
    "print(\"Validation data shape\", dataset_val.x.shape, dataset_val.y.shape)\n",
    "\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=config[\"training\"][\"batch_size\"], shuffle=True)\n",
    "val_dataloader = DataLoader(dataset_val, batch_size=config[\"training\"][\"batch_size\"], shuffle=True)\n",
    "\n",
    "#Iterate Through Data Loaders:\n",
    "print(\"Iterate Through Data Loaders:\")\n",
    "for batch_idx, (input_data, target_data) in enumerate(train_dataloader):\n",
    "        print(f\"Batch {batch_idx}: Input Shape: {input_data.shape}, Target Shape: {target_data.shape}\")\n",
    "        if batch_idx < 3:\n",
    "            print(f\"Batch {batch_idx}: Input Shape: {input_data.shape}, Target Shape: {target_data.shape}\")\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=64, num_layers=2, output_size=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        self.linear_1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm = nn.LSTM(hidden_layer_size, hidden_size=self.hidden_layer_size, num_layers=num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(num_layers*hidden_layer_size, output_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                 nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                 nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                 nn.init.orthogonal_(param)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "\n",
    "        # layer 1\n",
    "        x = self.linear_1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # LSTM layer\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "\n",
    "        # reshape output from hidden cell into [batch, features] for `linear_2`\n",
    "        x = h_n.permute(1, 0, 2).reshape(batchsize, -1) \n",
    "        \n",
    "        # layer 2\n",
    "        x = self.dropout(x)\n",
    "        predictions = self.linear_2(x)\n",
    "        return predictions[:,-1]\n",
    "\n",
    "def run_epoch(dataloader, is_training=False):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    if is_training:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    for idx, (x, y) in enumerate(dataloader):\n",
    "        if is_training:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        batchsize = x.shape[0]\n",
    "\n",
    "        x = x.to(config[\"training\"][\"device\"])\n",
    "        y = y.to(config[\"training\"][\"device\"])\n",
    "\n",
    "        out = model(x)\n",
    "        loss = criterion(out.contiguous(), y.contiguous())\n",
    "\n",
    "        if is_training:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss += (loss.detach().item() / batchsize)\n",
    "\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "    return epoch_loss, lr\n",
    "\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=config[\"training\"][\"batch_size\"], shuffle=True)\n",
    "\n",
    "#print(len(train_dataloader))\n",
    "for batch in train_dataloader:\n",
    "    # Process the batch\n",
    "    x, y = batch\n",
    "    #print(x.shape, y.shape)\n",
    "\n",
    "val_dataloader = DataLoader(dataset_val, batch_size=config[\"training\"][\"batch_size\"], shuffle=True)\n",
    "print(len(val_dataloader))\n",
    "for batch in val_dataloader:\n",
    "    # Process the batch\n",
    "    x, y = batch\n",
    "    #print(x.shape, y.shape)\n",
    "\n",
    "model = LSTMModel(input_size=config[\"model\"][\"input_size\"], hidden_layer_size=config[\"model\"][\"lstm_size\"], num_layers=config[\"model\"][\"num_lstm_layers\"], output_size=1, dropout=config[\"model\"][\"dropout\"])\n",
    "model = model.to(config[\"training\"][\"device\"])\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"training\"][\"learning_rate\"], betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=config[\"training\"][\"scheduler_step_size\"], gamma=0.1)\n",
    "\n",
    "for epoch in range(config[\"training\"][\"num_epoch\"]):\n",
    "    loss_train, lr_train = run_epoch(train_dataloader, is_training=True)\n",
    "    loss_val, lr_val = run_epoch(val_dataloader)\n",
    "    scheduler.step()\n",
    "    \n",
    "    print('Epoch[{}/{}] | loss train:{:.6f}, test:{:.6f} | lr:{:.6f}'\n",
    "              .format(epoch+1, config[\"training\"][\"num_epoch\"], loss_train, loss_val, lr_train))\n",
    "\n",
    "# here we re-initialize dataloader so the data doesn't shuffled, so we can plot the values by date\n",
    "\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=config[\"training\"][\"batch_size\"], shuffle=False)\n",
    "val_dataloader = DataLoader(dataset_val, batch_size=config[\"training\"][\"batch_size\"], shuffle=False)\n",
    "\n",
    "print(\"evaluate model-start\")\n",
    "model.eval()\n",
    "print(\"evaluate model-end\")\n",
    "\n",
    "# predict on the training data, to see how well the model managed to learn and memorize\n",
    "\n",
    "predicted_train = np.array([])\n",
    "\n",
    "for idx, (x, y) in enumerate(train_dataloader):\n",
    "    print(\"train_dataloader\")\n",
    "    x = x.to(config[\"training\"][\"device\"])\n",
    "    out = model(x)\n",
    "    out = out.cpu().detach().numpy()\n",
    "    predicted_train = np.concatenate((predicted_train, out))\n",
    "\n",
    "# predict on the validation data, to see how the model does\n",
    "\n",
    "predicted_val = np.array([])\n",
    "\n",
    "for idx, (x, y) in enumerate(val_dataloader):\n",
    "    print(\"val_dataloader\")\n",
    "    x = x.to(config[\"training\"][\"device\"])\n",
    "    out = model(x)\n",
    "    out = out.cpu().detach().numpy()\n",
    "    predicted_val = np.concatenate((predicted_val, out))\n",
    "\n",
    "# prepare data for plotting\n",
    "print(\"----plottings--------------\")\n",
    "to_plot_data_y_train_pred = np.zeros(num_data_points)\n",
    "to_plot_data_y_val_pred = np.zeros(num_data_points)\n",
    "\n",
    "to_plot_data_y_train_pred[config[\"data\"][\"window_size\"]:split_index+config[\"data\"][\"window_size\"]] = scaler.inverse_transform(predicted_train)\n",
    "to_plot_data_y_val_pred[split_index+config[\"data\"][\"window_size\"]:] = scaler.inverse_transform(predicted_val)\n",
    "\n",
    "to_plot_data_y_train_pred = np.where(to_plot_data_y_train_pred == 0, None, to_plot_data_y_train_pred)\n",
    "to_plot_data_y_val_pred = np.where(to_plot_data_y_val_pred == 0, None, to_plot_data_y_val_pred)\n",
    "\n",
    "# plots\n",
    "\n",
    "fig = figure(figsize=(25, 5), dpi=80)\n",
    "fig.patch.set_facecolor((1.0, 1.0, 1.0))\n",
    "plt.plot(data_date, data_close_price, label=\"Actual prices\", color=config[\"plots\"][\"color_actual\"])\n",
    "plt.plot(data_date, to_plot_data_y_train_pred, label=\"Predicted prices (train)\", color=config[\"plots\"][\"color_pred_train\"])\n",
    "plt.plot(data_date, to_plot_data_y_val_pred, label=\"Predicted prices (validation)\", color=config[\"plots\"][\"color_pred_val\"])\n",
    "plt.title(\"Compare predicted prices to actual prices\")\n",
    "xticks = [data_date[i] if ((i%config[\"plots\"][\"xticks_interval\"]==0 and (num_data_points-i) > config[\"plots\"][\"xticks_interval\"]) or i==num_data_points-1) else None for i in range(num_data_points)] # make x ticks nice\n",
    "x = np.arange(0,len(xticks))\n",
    "plt.xticks(x, xticks, rotation='vertical')\n",
    "plt.grid(which='major', axis='y', linestyle='--')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# predict the closing price of the next trading day\n",
    "\n",
    "model.eval()\n",
    "\n",
    "x = torch.tensor(data_x_unseen).float().to(config[\"training\"][\"device\"]).unsqueeze(0).unsqueeze(2) # this is the data type and shape required, [batch, sequence, feature]\n",
    "prediction = model(x)\n",
    "prediction = prediction.cpu().detach().numpy()\n",
    "\n",
    "# prepare plots\n",
    "\n",
    "plot_range = 10\n",
    "to_plot_data_y_val = np.zeros(plot_range)\n",
    "to_plot_data_y_val_pred = np.zeros(plot_range)\n",
    "to_plot_data_y_test_pred = np.zeros(plot_range)\n",
    "\n",
    "to_plot_data_y_val[:plot_range-1] = scaler.inverse_transform(data_y_val)[-plot_range+1:]\n",
    "to_plot_data_y_val_pred[:plot_range-1] = scaler.inverse_transform(predicted_val)[-plot_range+1:]\n",
    "\n",
    "to_plot_data_y_test_pred[plot_range-1] = scaler.inverse_transform(prediction)[0]\n",
    "to_plot_data_y_val = np.where(to_plot_data_y_val == 0, None, to_plot_data_y_val)\n",
    "to_plot_data_y_val_pred = np.where(to_plot_data_y_val_pred == 0, None, to_plot_data_y_val_pred)\n",
    "\n",
    "to_plot_data_y_test_pred = np.where(to_plot_data_y_test_pred == 0, None, to_plot_data_y_test_pred)\n",
    "\n",
    "# plot\n",
    "\n",
    "plot_date_test = data_date[-plot_range+1:]\n",
    "plot_date_test = pd.concat([plot_date_test, pd.Series([\"tomorrow\"])], ignore_index=True)\n",
    "\n",
    "fig = figure(figsize=(25, 5), dpi=80)\n",
    "fig.patch.set_facecolor((1.0, 1.0, 1.0))\n",
    "plt.plot(plot_date_test, to_plot_data_y_val, label=\"Actual prices\", marker=\".\", markersize=10, color=config[\"plots\"][\"color_actual\"])\n",
    "plt.plot(plot_date_test, to_plot_data_y_val_pred, label=\"Past predicted prices\", marker=\".\", markersize=10, color=config[\"plots\"][\"color_pred_val\"])\n",
    "plt.plot(plot_date_test, to_plot_data_y_test_pred, label=\"Predicted price for next day\", marker=\".\", markersize=20, color=config[\"plots\"][\"color_pred_test\"])\n",
    "plt.title(\"Predicted close price of the next trading day\")\n",
    "plt.grid(which='major', axis='y', linestyle='--')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Predicted close price of the next trading day:\", round(to_plot_data_y_test_pred[plot_range-1], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41295b81-9b86-4698-ad02-f0979f115d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64052e85-2221-4210-835b-c54e5ec98873",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
